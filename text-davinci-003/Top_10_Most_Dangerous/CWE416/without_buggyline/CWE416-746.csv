Original code,Plausible Program,Total Tokens,Sum Entropy,Mean Entropy
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { //add a kfree to prevent Use After Free kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { //add a kfree to prevent Use After Free kfree ( ucounts ) ; ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",331,-18.92544259202674,-0.05717656372213517
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",284,-4.099610884493501,-0.014435249593286974
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head_rcu ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; synchronize_rcu ( ) ; return ucounts ; }",312,-5.197676775712543,-0.0166592204349761
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } atomic_inc ( & ucounts -> count ) ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",284,-5.673465070687899,-0.019976989685520773
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { // CWE-416: Added this line to free resources allocated to ""new"" before using them kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",321,-36.53305546565862,-0.1138101416375658
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { // ADDED: Ensure new is freed kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { // ADDED: If count cannot be incremented, ensure new is freed if (ucounts != NULL && ucounts != new) { kfree(new); } ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",347,-39.857296471158875,-0.11486252585348379
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { kfree(new); ucounts = NULL; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",308,-9.732318825990752,-0.031598437746723225
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new , * tmp ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } tmp = ucounts ; } if ( ! atomic_add_unless ( & tmp -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",308,-12.434369163371704,-0.04037132845250553
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } else { if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",316,-4.733702255017131,-0.014980070427269402
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ucounts && atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; } else { spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } }",322,-14.327236084027595,-0.0444945220000857
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; ucounts = NULL; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",307,-9.4842804109067,-0.030893421533898048
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:"," 
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_set ( & new -> count , 1 ) ; } } else { if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",317,-10.799765932808295,-0.03406866224860661
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",284,-4.074182038640701,-0.01434571140366444
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",292,-6.013543300503137,-0.020594326371586088
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } else { if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",316,-4.733702255017131,-0.014980070427269402
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head_rcu ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { kfree(ucounts); ucounts = NULL; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; } 

The code has been fixed by replacing the hlist_add_head() call with hlist_add_head_rcu(). This ensures that the memory for the ucounts structure is properly freed, preventing a Use After Free vulnerability.",365,-34.316994165781146,-0.09401916209803053
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",284,-3.760905821209001,-0.013242626131017609
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { kfree(ucounts); ucounts = NULL; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",310,-4.923653793703901,-0.015882754173238388
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } //Add reference count to ucounts. atomic_inc(&ucounts->refcount); if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; } 
//Releasing the reference count after use.
void release_ucounts(struct ucounts *ucounts){ if(ucounts != NULL){ atomic_dec(&ucounts->refcount); if(atomic_read(&ucounts->refcount) == 0){ kfree(ucounts); } } }",394,-43.57455146363583,-0.11059530828333966
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { kfree(ucounts); ucounts = NULL; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",310,-4.923653793703901,-0.015882754173238388
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head_rcu ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; 

synchronize_rcu ( ) ; 
}",318,-14.137680169055239,-0.044458113739167415
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } atomic_inc ( & ucounts -> count ) ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",284,-5.475311939382399,-0.019279267392191546
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; /* Add reference count */ atomic_inc(&ucounts->count); } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { /* Drop the reference count */ atomic_dec(&ucounts->count); ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",334,-22.020444063100186,-0.06592947324281492
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc (&ucounts->count); } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",282,-8.898000542137,-0.03155319341183333
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) { spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { kfree ( ucounts ) ; ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",262,-4.396720487863798,-0.016781375907877094
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & new -> count ) ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",311,-7.3073485497321,-0.023496297587563022
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","

static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) { spin_lock_irq ( & ucounts_lock ) ; return NULL ; } new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",302,-13.762909300878631,-0.04557254735390275
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) { spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { hlist_del ( & ucounts -> node ) ; kfree ( ucounts ) ; ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",275,-5.164299210541252,-0.018779269856513642
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kmalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; kfree(ucounts); }",307,-8.800544184760831,-0.028666267702804008
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } else { if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",316,-4.606734385022471,-0.014578273370324276
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } else { // Add reference for ucounts before kfree kref_get ( & ucounts -> refcount ) ; kfree ( new ) ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { // Drop reference after failed atomic_add_unless kref_put ( & ucounts -> refcount ) ; ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",352,-33.94625517890628,-0.09643822494007465
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } else if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",314,-6.810461068926683,-0.021689366461549946
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) { spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",252,-4.694089422471647,-0.01862733897806209
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; // Added this line } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",288,-10.0183433626579,-0.03478591445367327
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; //increment here to prevent count from being 0 after lock release } } spin_unlock_irq ( & ucounts_lock ) ; //release lock after incrementing count return ucounts ; }",304,-41.35574716865031,-0.1360386420021392
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc( & ucounts -> count ); } } else { if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",315,-9.114447860571332,-0.028934755112924863
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & new -> count ) ; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",282,-7.2581120678705,-0.02573798605627837
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } // added an else statement if (!atomic_add_unless(&ucounts->count, 1, INT_MAX)) { // added kfree to prevent use-after-free kfree(ucounts); ucounts = NULL; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",323,-31.58617850664758,-0.09779002633637023
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; int refcount = 0 ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; refcount = atomic_inc_return ( & new -> count ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } else { refcount = atomic_inc_return ( & ucounts -> count ) ; } if ( refcount > INT_MAX ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",328,-17.76612608043684,-0.0541650185379172
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { spin_unlock_irq ( & ucounts_lock ) ; kfree ( ucounts ) ; ucounts = NULL ; } else { spin_unlock_irq ( & ucounts_lock ) ; } return ucounts ; }",330,-8.6448368552061,-0.026196475318806364
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) { spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { spin_unlock_irq ( & ucounts_lock ) ; kfree ( ucounts ) ; return NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",275,-5.03650780839364,-0.018314573848704146
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } atomic_inc ( & ucounts -> count ) ; //Increase count here to prevent use-after-free spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",294,-21.929698730466498,-0.07459081200838945
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; /* Atomic add unless to prevent use-after-free vulnerability */ atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",305,-22.80849195541434,-0.07478194083742407
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & new -> count ) ; // Add reference to the ucounts structure } } else { atomic_inc ( & ucounts -> count ) ; // Add reference to the ucounts structure } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }

// Add a reference to the ucounts structure in both cases (when a new ucounts structure is allocated and when ucounts is found in the list) to avoid the use after free vulnerability.",359,-57.170967204555694,-0.15925060502661753
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) { spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { hlist_del ( & ucounts -> node ) ; kfree ( ucounts ) ; spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",288,-5.927291927017928,-0.020580874746590025
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","

static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; // Increment reference count to prevent use-after-free. atomic_inc( & ucounts -> count ); } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",326,-18.656559938347588,-0.05722871146732389
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { kfree ( ucounts ) ; ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",311,-5.3808272264041,-0.017301695261749517
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } else { if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",316,-4.601409564212573,-0.014561422671558775
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) { spin_unlock_irq ( & ucounts_lock ) ; return NULL ; } new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { spin_unlock_irq ( & ucounts_lock ) ; kfree ( ucounts ) ; return NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",275,-5.049396660942039,-0.018361442403425594
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts *get_ucounts (struct user_namespace *ns, kuid_t uid) { struct hlist_head *hashent = ucounts_hashentry(ns, uid); struct ucounts *ucounts, *new; 
 spin_lock_irq(&ucounts_lock); ucounts = find_ucounts(ns, uid, hashent); if (!ucounts) { spin_unlock_irq(&ucounts_lock); new = kzalloc(sizeof(*new), GFP_KERNEL); if (!new) return NULL; new->ns = ns; new->uid = uid; atomic_set(&new->count, 1); hlist_add_head(&new->node, hashent); 
 spin_lock_irq(&ucounts_lock); ucounts = find_ucounts(ns, uid, hashent); if (ucounts) { kfree(new); atomic_inc(&ucounts->count); } else { ucounts = new; } } else { if (!atomic_add_unless(&ucounts->count, 1, INT_MAX)) ucounts = NULL; } spin_unlock_irq(&ucounts_lock); return ucounts;
}",295,-35.82720916930412,-0.12144816667560718
