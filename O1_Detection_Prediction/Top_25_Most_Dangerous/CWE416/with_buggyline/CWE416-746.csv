Original code,Plausible Program,Number of Bugs Present,Total Tokens,Sum Entropy,Mean Entropy
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ucounts ) { atomic_inc ( & ucounts -> count ) ; } else { ucounts = NULL ; } <EndBug> spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,309,-6.98858178154445,-0.02261676951956133
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,312,-4.392478492842892,-0.014078456707829784
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,310,-5.627133581960489,-0.018152043812775773
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; <StartBug> atomic_inc ( & ucounts -> count ) ; <EndBug> } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,292,-4.591189619277153,-0.01572325212081217
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; <StartBug> refcount_inc ( & ucounts -> refcount ) ; <EndBug> } } <StartBug> if ( ! refcount_inc_not_zero ( & ucounts -> refcount ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,329,-8.61726740439685,-0.02619230214102386
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & new -> count ) ; } } else { atomic_inc ( & ucounts -> count ) ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,297,-9.59918544287395,-0.03232048970664629
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ucounts ) atomic_inc ( & ucounts -> count ) ; <EndBug> else ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,305,-7.132563018247953,-0.023385452518845745
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-3.0164428647970487,-0.010621277692947354
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-3.025614630577248,-0.010653572642877635
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-3.0164428647970487,-0.010621277692947354
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-3.025614630577248,-0.010653572642877635
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc_not_zero ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,288,-5.34718489852565,-0.01856661423099184
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ucounts ) atomic_inc ( & ucounts -> count ) ; else ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,297,-6.145101228825747,-0.020690576528032818
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> refcount_inc(&ucounts->refcount); <EndBug> if ( ! refcount_inc_not_zero ( & ucounts -> refcount ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,319,-14.057347989699657,-0.04406692159780457
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-3.48631282561015,-0.012275749385951232
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc_return ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,286,-7.731865082195653,-0.027034493294390394
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,316,-4.923154191679742,-0.015579601872404246
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ucounts && atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,308,-6.261532196675848,-0.020329649989207298
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> else if ( atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,313,-9.050498394004544,-0.028915330332282888
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; <StartBug> refcount_set ( & new -> count , 1 ) ; <EndBug> } } else { <StartBug> refcount_inc ( & ucounts -> count ) ; <EndBug> if ( refcount_read ( & ucounts -> count ) == 0 ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,340,-15.52020060753325,-0.04564764884568603
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ucounts && atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,308,-5.994321881717,-0.019462084031548702
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( atomic_inc_not_zero ( & ucounts -> count ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,296,-4.48957353779825,-0.015167478168237332
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } <StartBug> if ( ucounts ) atomic_inc ( & ucounts -> count ) ; <EndBug> } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,298,-5.4192441535403,-0.018185383065571478
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( ucounts ) atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,290,-6.3265155590099456,-0.021815570893137744
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ucounts && ! atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,309,-5.922959179584099,-0.019168152684738184
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-4.009132480562051,-0.014116663663950884
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } <StartBug> if ( ucounts && atomic_inc_not_zero ( & ucounts -> count ) ) { <EndBug> spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; } else { spin_unlock_irq ( & ucounts_lock ) ; kfree ( ucounts ) ; return NULL ; } }",2,333,-10.661822992688995,-0.032017486464531515
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ;<StartBug> refcount_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! refcount_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,315,-9.713738948459481,-0.03083726650304597
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> atomic_inc ( & ucounts -> count ) ; <EndBug> spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,300,-3.585151148899839,-0.01195050382966613
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:"," 
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) {
	struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ;
	struct ucounts * ucounts , * new ;
	spin_lock_irq ( & ucounts_lock ) ;
	ucounts = find_ucounts ( ns , uid , hashent ) ;
	if ( ! ucounts ) {
		spin_unlock_irq ( & ucounts_lock ) ;
		new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ;
		if ( ! new ) return NULL ;
		new -> ns = ns ;
		new -> uid = uid ; 
		atomic_set ( & new -> count , 0 ) ; 
		spin_lock_irq ( & ucounts_lock ) ;
		ucounts = find_ucounts ( ns , uid , hashent ) ;
		if ( ucounts ) {
			kfree ( new ) ;
		} else {
			hlist_add_head ( & new -> node , hashent ) ;
			ucounts = new ;
		}
	}
	if ( ucounts && atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) { // Check if ucounts is valid before incrementing the count
		ucounts = NULL ;
	}
	spin_unlock_irq ( & ucounts_lock ) ;
	return ucounts ;
}",2,391,-38.540454475826294,-0.09856893727832812
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> refcount_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! refcount_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,315,-3.7655071814760426,-0.011953991052304897
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { //add head after setting count hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,290,-31.362786730178545,-0.10814754044889154
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ucounts ) atomic_inc ( & ucounts -> count ) ; <EndBug> else ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,305,-7.073584283151102,-0.02319207961688886
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } else { if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,316,-6.541890880054529,-0.020702186329286484
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; new = NULL; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,288,-10.584467779731703,-0.036751624235179525
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> atomic_inc ( & ucounts -> count ) ; <EndBug> spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,300,-3.734405630007198,-0.01244801876669066
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-3.1272972039715006,-0.011011609873139086
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } <StartBug> if ( ucounts ) atomic_inc ( & ucounts -> count ) ; <EndBug> } else { <StartBug> if ( atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,333,-10.278520245892,-0.03086642716484084
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } } if ( ! atomic_inc_not_zero ( & ucounts -> count ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,309,-8.288576953458604,-0.02682387363578836
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; atomic_inc ( & new -> count ) ; ucounts = new ; } } if ( ! atomic_inc_not_zero ( & ucounts -> count ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,307,-8.838504057547297,-0.028789915496896735
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if (u counts) { atomic_inc ( & ucounts -> count ) ; } else { ucounts = NULL ; } <EndBug> spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,308,-19.1692435210688,-0.06223780363983377
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } <StartBug> refcount_inc(&ucounts->refcount); <EndBug> } if ( ! refcount_inc_not_zero ( & ucounts -> refcount ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,319,-13.791511146466494,-0.043233577261650454
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; atomic_inc ( & ucounts -> count ) ; } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,320,-10.618249608266606,-0.03318203002583314
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> atomic_inc ( & ucounts -> count ) ; <EndBug> spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,300,-3.734405630007198,-0.01244801876669066
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 0 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } if ( ucounts ) { <StartBug> atomic_inc ( & ucounts -> count ) ; <EndBug> } spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,299,-11.168803117490706,-0.037353856580236475
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } if ( atomic_inc_not_zero ( & ucounts -> count ) ) ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,296,-4.491811602173971,-0.015175039196533686
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,312,-4.201579993342443,-0.01346660254276424
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 1 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_inc_not_zero ( & ucounts -> count ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,313,-6.307605657116748,-0.02015209475117172
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-2.97371303915439,-0.010470820560402782
"The Original Code is vulnerable to CWE-416: Use After Free. Fix the code between <StartBug> and <EndBug> to fix the vulnerability.
Original Code:
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; <StartBug> atomic_set ( & new -> count , 0 ) ; <EndBug> spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } <StartBug> if ( ! atomic_add_unless ( & ucounts -> count , 1 , INT_MAX ) ) <EndBug> ucounts = NULL ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }
Fixed code:","
static struct ucounts * get_ucounts ( struct user_namespace * ns , kuid_t uid ) { struct hlist_head * hashent = ucounts_hashentry ( ns , uid ) ; struct ucounts * ucounts , * new ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ! ucounts ) { spin_unlock_irq ( & ucounts_lock ) ; new = kzalloc ( sizeof ( * new ) , GFP_KERNEL ) ; if ( ! new ) return NULL ; new -> ns = ns ; new -> uid = uid ; atomic_set ( & new -> count , 1 ) ; spin_lock_irq ( & ucounts_lock ) ; ucounts = find_ucounts ( ns , uid , hashent ) ; if ( ucounts ) { kfree ( new ) ; } else { hlist_add_head ( & new -> node , hashent ) ; ucounts = new ; } } atomic_inc ( & ucounts -> count ) ; spin_unlock_irq ( & ucounts_lock ) ; return ucounts ; }",2,284,-3.005773926650649,-0.010583711009333271
